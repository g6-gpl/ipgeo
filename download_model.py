from huggingface_hub import snapshot_download
from transformers import AutoTokenizer, AutoModel

import os

def download_model_hub(model_id, local_dir):
    """–°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É—è huggingface_hub"""
    
    print(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {model_id}")
    
    # –°–∫–∞—á–∞—Ç—å –≤—Å—é –º–æ–¥–µ–ª—å
    snapshot_download(
        repo_id=model_id,
        local_dir=local_dir,
        local_dir_use_symlinks=False,
        resume_download=True,
        allow_patterns=["*.json", "*.model", "*.bin", "*.txt", "config.json"]
    )
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ –≤: {local_dir}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
# download_model_hub(
#     "microsoft/deberta-v3-base",
#     "./models/deberta-v3-base-full"
# )

def test_local_model(model_path):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç"""
    
    try:
        print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å: {model_path}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModel.from_pretrained(model_path)
        
        # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        text = "Test input for model verification"
        inputs = tokenizer(text, return_tensors="pt")
        
        outputs = model(**inputs)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç! Output shape: {outputs.last_hidden_state.shape}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∫–∞—á–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
test_local_model("./models/deberta-v3-base-full")